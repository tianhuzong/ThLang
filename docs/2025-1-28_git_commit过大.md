# Git中commit了过大文件的解决办法
## ---记一次git由于历史提交包含过大体积文件而上传不去github的修复的心得

## 事件背景
在2025-1-27日,为了验证原本的分支的内容的错误是否由于std::unique_ptr所有权产生的问题而使用shared_ptr进行测试,在完成shared_ptr版本的测试后,我打算将这个分支上传到github,但由于往次commit中我使用valgrind产生的临时文件超过了github的限制100MB并且我没有删除,导致无法上传到github,为此我寻找了很多花里胡哨的办法,最后一位大佬用最简单粗暴的办法帮我解决.

## 问题再现
**注:问题内容为示例问题,并非与当时产生的问题完全一致**
以下是示例的提交记录:

| 哈希值 | 提交记录 |
| :---: | :---: |
|...bb38 | 这是一个message |
|...677d | 这是一个message |
|...adf5 | 这是一个message |
|...0134 | 这是一个message |
| ... | ...省略10个提交 |

假设第二个提交还正常,第三个提交时就包含了过大的文件,而往后的十几个提交都没有删掉,即使在最新的提交删掉了大文件,我们也无法上传到github.

## 解决办法
解决思路:用生成大文件前的分支/提交(即上文的第二个提交)覆盖现在的分支,然后再用备份的文件进行覆盖.
### 步骤一：备份文件
把你修改的文件备份一下(与第二个提交有变化的文件,或者直接备份全部文件)
### 步骤二：回退版本
使用`git reset --hard`命令强制回退到那个没有大文件的提交.
**注意：使用这个命令前要确认已经备份好文件！使用这个命令前要确认已经备份好文件！！使用这个命令前要确认已经备份好文件！！！**
假设我们要回退的那个提交的hash为`abcdef`(以实际的hash值为准,可以使用`git log来查看`),我们运行命令来回退:
```bash
git reset --hard abcdef
# 使用这个命令前要确认已经备份好文件！
```
### 步骤三：覆盖文件
在版本回退完后,我们使用先前备份的文件覆盖当前仓库,如果你比较有耐心的话可以一个文件一个文件去覆盖.
### 步骤四：检查
在覆盖完成后,我们就要检查是否当前仓库是否还存在过大文件?是否有忘记覆盖的文件?
### 步骤五：上传到云端
在完成上述步骤后,我们就可以将仓库push到github了!